{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6685900\n",
      "['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "review = pd.read_json('/users/tr.lisusi/yelp_dataset/round13/review.json', lines=True)\n",
    "print(len(review))\n",
    "print(list(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_date = review.drop(['cool','funny','review_id','stars','text','useful'],axis = 1)\n",
    "del review\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "uid_map = defaultdict(count().__next__)\n",
    "iid_map = defaultdict(count().__next__)\n",
    "uids = np.array([uid_map[uid] for uid in user_item_date[\"user_id\"].values ], dtype=np.int32)\n",
    "iids = np.array([iid_map[iid] for iid in user_item_date[\"business_id\"].values ], dtype=np.int32)\n",
    "\n",
    "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
    "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
    "\n",
    "def digital(data):\n",
    "    data['business_id'] = iid_map[data['business_id']]\n",
    "    data['user_id'] = uid_map[data['user_id']]\n",
    "    return data\n",
    "\n",
    "user_item_date = user_item_date.apply(digital,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1584133\n",
      "6199717\n",
      "4615584\n"
     ]
    }
   ],
   "source": [
    "\"\"\"remove all venues with less than 10 checkins\"\"\"\n",
    "lesscheckin_filter = user_item_date.groupby('business_id').filter(lambda x: len(x)>=10)\n",
    "\"\"\"ground truth for all users\"\"\"\n",
    "ground = lesscheckin_filter.sort_values('date').groupby('user_id').tail(1)\n",
    "\"\"\"remove all checkin from ground truth since many users have several checkins on ground truth venue\"\"\"\n",
    "counter = 0\n",
    "index = []\n",
    "all_user = list(ground['user_id'])\n",
    "for u in all_user:\n",
    "    ground_venue = float(ground.loc[ground['user_id']==u]['business_id'])\n",
    "    other_venues = list(lesscheckin_filter.loc[lesscheckin_filter['user_id']==u]['business_id'])\n",
    "    if ground_venue in other_venues:\n",
    "        index.extend(list(lesscheckin_filter.loc[(lesscheckin_filter['user_id']==u) \n",
    "                                              & (lesscheckin_filter['business_id']==ground_venue)].index.values))\n",
    "    \n",
    "    counter+=1\n",
    "    if counter%100000==0:print(counter)\n",
    "\n",
    "\n",
    "print(len(index))\n",
    "print(len(lesscheckin_filter))\n",
    "new_dataset = lesscheckin_filter.drop(index)\n",
    "print(len(new_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.interactions import Interactions\n",
    "import torch\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "import time\n",
    "from spotlight.losses import regression_loss\n",
    "from spotlight.torch_utils import cpu, gpu, minibatch, set_seed, shuffle\n",
    "from spotlight.losses import bpr_loss\n",
    "from spotlight.sampling import sample_items\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "train = Interactions(user_ids=uids, item_ids=iids)\n",
    "uids = np.array(new_dataset[\"user_id\"].values , dtype=np.int32)\n",
    "iids = np.array(new_dataset[\"business_id\"].values , dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.18935772559279646\n",
      "Epoch 1: loss 0.07945726898788354\n",
      "Epoch 2: loss 0.04951970218730901\n",
      "Epoch 3: loss 0.03778654812117079\n",
      "Epoch 4: loss 0.03213484586299703\n",
      "Epoch 5: loss 0.028974542955152935\n",
      "Epoch 6: loss 0.026932720234597977\n",
      "Epoch 7: loss 0.025483417464757182\n",
      "Epoch 8: loss 0.024291944306039048\n",
      "Epoch 9: loss 0.023314035158883882\n",
      "Training took 1767 seconds, gpu=True \n"
     ]
    }
   ],
   "source": [
    "model = ImplicitFactorizationModel(n_iter=10,embedding_dim=10,use_cuda=True, loss = 'bpr')\n",
    "\n",
    "current = time.time()\n",
    "model.fit(train, verbose=True)\n",
    "\n",
    "end = time.time()\n",
    "diff = end - current\n",
    "print(\"Training took %d seconds, gpu=%s \"% (diff, torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543938\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "unique_user = list(ground['user_id'])\n",
    "print(len(unique_user))\n",
    "unique_business = list(np.arange(max(new_dataset['business_id'])+1))\n",
    "testset = {}\n",
    "for user in unique_user:\n",
    "    \n",
    "    visited_venues = list(new_dataset.loc[new_dataset['user_id']==user,'business_id'])\n",
    "    unvisited_venues = sample(unique_business,100)\n",
    "    \n",
    "    while len(set(unvisited_venues)&set(visited_venues))!=0:\n",
    "        unvisited_venues = sample(unique_business,100)\n",
    "        \n",
    "    testset[user] = unvisited_venues\n",
    "    if len(testset)%100000 ==0: print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reconstruct ground as dict\"\"\"\n",
    "adder=0\n",
    "dict_ground = {}\n",
    "for index, row in ground.iterrows():\n",
    "    dict_ground[row['user_id']] = row['business_id']\n",
    "    adder+=1\n",
    "    if adder%100000==0:print(adder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543938\n",
      "100000\n",
      "0.8270928630655703\n",
      "200000\n",
      "0.8206537771145014\n",
      "300000\n",
      "0.8185957975306104\n",
      "400000\n",
      "0.8171492892529542\n",
      "500000\n",
      "0.8155773632541821\n",
      "600000\n",
      "0.8136716741519692\n",
      "700000\n",
      "0.811778670037556\n",
      "800000\n",
      "0.8096782874107062\n",
      "900000\n",
      "0.807481696896387\n",
      "1000000\n",
      "0.8051780380493313\n",
      "1100000\n",
      "0.8026126179537212\n",
      "1200000\n",
      "0.799910775769902\n",
      "1300000\n",
      "0.7969237701443292\n",
      "1400000\n",
      "0.7938629677780756\n",
      "1500000\n",
      "0.7898267901322226\n",
      "0.7879669573470498\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def position_finder (predictions,groundtruth):\n",
    "    position = 1\n",
    "    for i in predictions:\n",
    "        if groundtruth < i:\n",
    "            position+=1\n",
    "        else:\n",
    "            break\n",
    "    return position\n",
    "\n",
    "    \n",
    "def ndcg_calculator(user_id , model,bound,ground):\n",
    "    ndcg = 0\n",
    "    rel = 1\n",
    "    position = 0\n",
    "    unvisited_predictions = np.sort(model.predict(user_id)[testset[user_id]])[::-1][0:10]\n",
    "    ground_venue = int(ground[user_id])\n",
    "    if ground_venue <= bound:\n",
    "        ground_prediction = model.predict(user_id)[ground_venue]\n",
    "        if ground_prediction >= min(unvisited_predictions):\n",
    "            position = position_finder(unvisited_predictions,ground_prediction)\n",
    "            ndcg = rel/(np.log2(position + 1))\n",
    "        else:\n",
    "            ndcg = 0\n",
    "    else:\n",
    "        ndcg = 0\n",
    "    return ndcg\n",
    "adder = 0\n",
    "NDCG = []\n",
    "print(len(testset.keys()))\n",
    "max_business = max(iids)\n",
    "max_user = max(uids)\n",
    "for u in list(testset.keys()):\n",
    "    if u <=max_user:\n",
    "        NDCG.append(ndcg_calculator(u,model,max_business,dict_ground))\n",
    "    adder+=1\n",
    "    if adder%100000==0:\n",
    "        print(adder)\n",
    "        print(np.array(NDCG).mean())\n",
    "print(np.array(NDCG).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
